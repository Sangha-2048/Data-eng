https://github.com/ziritrion/dataeng-zoomcamp/blob/main/notes/1_intro.md

Docker basic concepts
(Video source)

Docker is a containerization software that allows us to isolate software in a similar way to virtual machines but in a much leaner way.

A Docker image is a snapshot of a container that we can define to run our software, or in this case our data pipelines. By exporting our Docker images to Cloud providers such as Amazon Web Services or Google Cloud Platform we can run our containers there.

Docker provides the following advantages:

Reproducibility
Local experimentation
Integration tests (CI/CD)
Running pipelines on the cloud (AWS Batch, Kubernetes jobs)
Spark (analytics engine for large-scale data processing)
Serverless (AWS Lambda, Google functions)
Docker containers are stateless: any changes done inside a container will NOT be saved when the container is killed and started again. This is an advantage because it allows us to restore any container to its initial state in a reproducible manner, but you will have to store data elsewhere if you need to do so; a common way to do so is with volumes.

Note: you can learn more about Docker and how to set it up on a Mac in this link. You may also be interested in a Docker reference cheatsheet.

Creating a custom pipeline with Docker
(Video source)

Let's create an example pipeline. We will create a dummy pipeline.py Python script that receives an argument and prints it.

import sys
import pandas # we don't need this but it's useful for the example

# print arguments
print(sys.argv)

# argument 0 is the name os the file
# argumment 1 contains the actual first argument we care about
day = sys.argv[1]

# cool pandas stuff goes here

# print a sentence with the argument
print(f'job finished successfully for day = {day}')
We can run this script with python pipeline.py <some_number> and it should print 2 lines:

['pipeline.py', '<some_number>']
job finished successfully for day = <some_number>
Let's containerize it by creating a Docker image. Create the folllowing Dockerfile file:

# base Docker image that we will build on
FROM python:3.9.1

# set up our image by installing prerequisites; pandas in this case
RUN pip install pandas

# set up the working directory inside the container
WORKDIR /app
# copy the script to the container. 1st name is source file, 2nd is destination
COPY pipeline.py pipeline.py

# define what to do first when the container runs
# in this example, we will just run the script
ENTRYPOINT ["python", "pipeline.py"]
Let's build the image:

docker build -t test:pandas .
The image name will be test and its tag will be pandas. If the tag isn't specified it will default to latest.
We can now run the container and pass an argument to it, so that our pipeline will receive it:

docker run -it test:pandas some_number
You should get the same output you did when you ran the pipeline script by itself.

Note: these instructions asume that pipeline.py and Dockerfile are in the same directory. The Docker commands should also be run from the same directory as these files.